{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kaggle_bandit.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOJ6ZHwiwWUm/8ZmDVk2zku"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"U0f8LDHRCYvA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610615208327,"user_tz":-540,"elapsed":25074,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"51ab46dd-825e-4111-c43b-0c1cf768b347"},"source":["from google.colab import drive\r\n","drive.mount(\"/content/drive\")\r\n","%cd /content/drive/MyDrive/'Colab Notebooks'/RL_Python\r\n","!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks/RL_Python\n","baby-steps-of-rl-ja\t    epsilon_TS_gamma.py  RL_Python.ipynb\n","bayesian_ucb.py\t\t    experiment.py\t thompson_sampling.py\n","bayesian_ucb_TS_epsilon.py  Kaggle_bandit.ipynb  ucb_decay.py\n","bayesian_ucb_TS.py\t    pull_same.py\n","epsilon_TS_beta.py\t    pull_same_TS_ucb.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jpSGDiyCTkgZ"},"source":["!cat ./baby-steps-of-rl-ja/EL/actor_critic.py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3KtIwXXkG35W"},"source":["＊提出"]},{"cell_type":"markdown","metadata":{"id":"D7W1aRQ2gLpu"},"source":["＊実験"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oIngLkOYJzGZ","executionInfo":{"status":"ok","timestamp":1610533181586,"user_tz":-540,"elapsed":657,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"896dd2ec-0b6c-46bd-ec77-6935ce65b1c9"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"skh_WSpI0iRY","executionInfo":{"status":"ok","timestamp":1610204293938,"user_tz":-540,"elapsed":663,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"844d1717-c784-462d-e9e3-f83769e5203c"},"source":["%%writefile epsilon_TS_beta.py\r\n","\r\n","import json\r\n","import numpy as np\r\n","import pandas as pd\r\n","import random\r\n","\r\n","bandit_state = None\r\n","total_reward = 0\r\n","epsilon = 0.12\r\n","    \r\n","def epsilon_TS_beta_agent (observation, configuration):\r\n","    global history, history_bandit\r\n","\r\n","    step = 1.0 #you can regulate exploration / exploitation balacne using this param\r\n","    decay_rate = 0.97 # how much do we decay the win count after each call\r\n","    \r\n","    global bandit_state,total_reward, epsilon\r\n","\r\n","    if observation.step == 0:\r\n","        # initial bandit state\r\n","        bandit_state = [[1,1] for i in range(configuration[\"banditCount\"])]\r\n","    else:       \r\n","        # updating bandit_state using the result of the previous step\r\n","        last_reward = observation[\"reward\"] - total_reward\r\n","        total_reward = observation[\"reward\"]\r\n","        \r\n","        # we need to understand who we are Player 1 or 2\r\n","        last_bandit = observation.lastActions[observation.agentIndex]\r\n","\r\n","        if last_reward > 0:\r\n","            bandit_state[last_bandit][0] += last_reward * step\r\n","        else:\r\n","            bandit_state[last_bandit][1] += step\r\n","        \r\n","        bandit_state[observation.lastActions[0]][0] = (bandit_state[observation.lastActions[0]][0] - 1) * decay_rate + 1\r\n","        bandit_state[observation.lastActions[1]][0] = (bandit_state[observation.lastActions[1]][0] - 1) * decay_rate + 1\r\n","\r\n","\r\n","    if random.random() < epsilon:\r\n","      if observation.step < 1000:\r\n","        return random.choice(range(configuration[\"banditCount\"]))\r\n","#     generate random number from Beta distribution for each agent and select the most lucky one\r\n","    best_proba = -1\r\n","    best_agent = None\r\n","    for k in range(configuration[\"banditCount\"]):\r\n","        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\r\n","        if proba > best_proba:\r\n","            best_proba = proba\r\n","            best_agent = k\r\n","\r\n","    return best_agent"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting epsilon_TS_beta.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C12PCYYOML4W","executionInfo":{"status":"ok","timestamp":1610285165650,"user_tz":-540,"elapsed":967,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"90661fee-d944-40cc-e2ed-ef2b63fa1094"},"source":["%%writefile bayesian_ucb_TS.py\r\n","\r\n","import json\r\n","import numpy as np\r\n","import pandas as pd\r\n","import random\r\n","import math\r\n","from scipy.stats import beta\r\n","\r\n","bandit_state = None\r\n","total_reward = 0\r\n","epsilon = 0.1\r\n","c = 3\r\n","    \r\n","def bayesian_ucb_TS_agent (observation, configuration):\r\n","    global history, history_bandit\r\n","\r\n","    step = 1.0 #you can regulate exploration / exploitation balacne using this param\r\n","    decay_rate = 0.97 # how much do we decay the win count after each call\r\n","    \r\n","    global bandit_state,total_reward, epsilon\r\n","\r\n","    if observation.step == 0:\r\n","        # initial bandit state\r\n","        bandit_state = [[1,1] for i in range(configuration[\"banditCount\"])]\r\n","    else:       \r\n","        # updating bandit_state using the result of the previous step\r\n","        last_reward = observation[\"reward\"] - total_reward\r\n","        total_reward = observation[\"reward\"]\r\n","        \r\n","        # we need to understand who we are Player 1 or 2\r\n","        last_bandit = observation.lastActions[observation.agentIndex]\r\n","\r\n","        if last_reward > 0:\r\n","            bandit_state[last_bandit][0] += last_reward * step\r\n","        else:\r\n","            bandit_state[last_bandit][1] += step\r\n","        \r\n","        bandit_state[observation.lastActions[0]][0] = (bandit_state[observation.lastActions[0]][0] - 1) * decay_rate + 1\r\n","        bandit_state[observation.lastActions[1]][0] = (bandit_state[observation.lastActions[1]][0] - 1) * decay_rate + 1\r\n","\r\n","#     generate random number from Beta distribution for each agent and select the most lucky one        \r\n","    best_proba = -1\r\n","    best_agent = None\r\n","    for k in range(configuration[\"banditCount\"]):\r\n","        sigma = beta.std(bandit_state[k][0],bandit_state[k][1])\r\n","        count = (bandit_state[k][0]+bandit_state[k][1])\r\n","        if observation.step < 500:\r\n","          #r = np.random.beta(bandit_state[k][0],bandit_state[k][1])\r\n","          #r = bandit_state[k][0] / (bandit_state[k][0]+bandit_state[k][1])\r\n","          #sigma = beta.std(bandit_state[k][0],bandit_state[k][1])\r\n","          #count = (bandit_state[k][0]+bandit_state[k][1])\r\n","          #print(\"r:\\t\\t\", r)\r\n","          #print(\"sigma:\\t\\t\", sigma)\r\n","          #print(\"1/count:\\t\\t\", 1/count)\r\n","          #proba = r + sigma + 1/count\r\n","          #print(proba)\r\n","          proba = bandit_state[k][0]/count + sigma * c\r\n","        #if random.random() < epsilon:\r\n","        #  proba = np.random.beta(bandit_state[k][0],bandit_state[k][1]) + beta.std(bandit_state[k][0], bandit_state[k][1]) * c\r\n","        else:\r\n","          proba = np.random.beta(bandit_state[k][0],bandit_state[k][1]) + bandit_state[k][0]/count + sigma * c\r\n","        if proba > best_proba:\r\n","            best_proba = proba\r\n","            best_agent = k\r\n","\r\n","    return best_agent"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting bayesian_ucb_TS.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rnzl4_tGD_o9"},"source":["# observation = {\r\n","#     'remainingOverageTime': 60,\r\n","#     'agentIndex': 1, # 0 or 1\r\n","#     'reward': 92, # total reward\r\n","#     'step': 184, # [0-1999]\r\n","#     'lastActions': [84, 94]\r\n","# }\r\n","\r\n","# configuration:\r\n","# {'episodeSteps': 2000,\r\n","#  'actTimeout': 0.25,\r\n","#  'runTimeout': 1200,\r\n","#  'banditCount': 100,\r\n","#  'decayRate': 0.97,\r\n","#  'sampleResolution': 100}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLLGqJ7IJSDE","executionInfo":{"status":"ok","timestamp":1610615231140,"user_tz":-540,"elapsed":2344,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"1edd5ca9-a2b7-4f77-f001-12a5700be7bc"},"source":["%%writefile pull_same_TS_ucb.py\r\n","\r\n","import numpy as np\r\n","import pandas as pd\r\n","import random, os, datetime, math\r\n","from collections import defaultdict\r\n","from scipy.stats import beta\r\n","\r\n","total_reward = 0\r\n","bandit_dict = {}\r\n","\r\n","def set_seed(my_seed=42):\r\n","    os.environ['PYTHONHASHSEED'] = str(my_seed)\r\n","    random.seed(my_seed)\r\n","    np.random.seed(my_seed)\r\n","\r\n","def get_next_bandit():\r\n","    best_bandit = 0\r\n","    best_bandit_expected = 0\r\n","    for bnd in bandit_dict:\r\n","        # beta.std(bandit_dict[bnd]['win'], bandit_dict[bnd]['loss']*math.pow(1.03, bandit_dict[bnd]['win']+bandit_dict[bnd]['loss']+bandit_dict[bnd]['opp']))\\\r\n","        expect = beta.std(bandit_dict[bnd]['win'], bandit_dict[bnd]['loss']*math.pow(1.03, bandit_dict[bnd]['win']+bandit_dict[bnd]['loss']+bandit_dict[bnd]['opp']))\\\r\n","                + (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5 + bandit_dict[bnd]['op_continue']) \\\r\n","                / (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp']) \\\r\n","                * math.pow(0.97, bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])\r\n","        #print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\r\n","        #print(expect)\r\n","        #print(np.random.beta(bandit_dict[bnd]['win'],bandit_dict[bnd]['loss']))\r\n","        #print(beta.std(bandit_dict[bnd]['win'],bandit_dict[bnd]['loss']) * 3)\r\n","        if expect > best_bandit_expected:\r\n","            best_bandit_expected = expect\r\n","            best_bandit = bnd\r\n","    return best_bandit\r\n","\r\n","def get_next_bandit_TS():\r\n","    best_bandit = 0\r\n","    best_bandit_expected = 0\r\n","    for bnd in bandit_dict:\r\n","        # beta.std(bandit_dict[bnd]['win']*math.pow(0.97, bandit_dict[bnd]['win']+bandit_dict[bnd]['loss']+bandit_dict[bnd]['opp']), bandit_dict[bnd]['loss']) * 3 + \\ \r\n","        # + beta.std(bandit_dict[bnd]['win'],bandit_dict[bnd]['loss']) * 3 \\\r\n","        expect = np.random.beta(bandit_dict[bnd]['win'], bandit_dict[bnd]['loss']) \\\r\n","                + (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5 + bandit_dict[bnd]['op_continue']) \\\r\n","                / (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp']) \\\r\n","                * math.pow(0.97, bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])\r\n","        if expect > best_bandit_expected:\r\n","            best_bandit_expected = expect\r\n","            best_bandit = bnd\r\n","    return best_bandit\r\n","\r\n","my_action_list = []\r\n","op_action_list = []\r\n","\r\n","op_continue_cnt_dict = defaultdict(int)\r\n","\r\n","def multi_armed_probabilities(observation, configuration):\r\n","    global total_reward, bandit_dict\r\n","\r\n","    my_pull = 0\r\n","    if 0 == observation['step']:\r\n","        set_seed()\r\n","        total_reward = 0\r\n","        bandit_dict = {}\r\n","        for i in range(configuration['banditCount']):\r\n","            bandit_dict[i] = {'win': 1, 'loss': 1, 'opp': 0, 'my_continue': 0, 'op_continue': 0}\r\n","    else:\r\n","        last_reward = observation['reward'] - total_reward\r\n","        total_reward = observation['reward']\r\n","        \r\n","        my_idx = observation['agentIndex']\r\n","        my_last_action = observation['lastActions'][my_idx]\r\n","        op_last_action = observation['lastActions'][1-my_idx]\r\n","        \r\n","        my_action_list.append(my_last_action)\r\n","        op_action_list.append(op_last_action)\r\n","        \r\n","        if 0 < last_reward:\r\n","            bandit_dict[my_last_action]['win'] = bandit_dict[my_last_action]['win'] +1\r\n","        else:\r\n","            bandit_dict[my_last_action]['loss'] = bandit_dict[my_last_action]['loss'] +1\r\n","        bandit_dict[op_last_action]['opp'] = bandit_dict[op_last_action]['opp'] +1\r\n","        \r\n","        if observation['step'] >= 3:\r\n","            if my_action_list[-1] == my_action_list[-2]:\r\n","                bandit_dict[my_last_action]['my_continue'] += 1\r\n","            else:\r\n","                bandit_dict[my_last_action]['my_continue'] = 0\r\n","            if op_action_list[-1] == op_action_list[-2]:\r\n","                bandit_dict[op_last_action]['op_continue'] += 1\r\n","            else:\r\n","                bandit_dict[op_last_action]['op_continue'] = 0\r\n","        \r\n","        if last_reward > 0:\r\n","            my_pull = my_last_action\r\n","        else:\r\n","            if observation['step'] >= 4:\r\n","                if (my_action_list[-1] == my_action_list[-2]) and (my_action_list[-1] == my_action_list[-3]):\r\n","                    # if random.random() < 0.6:\r\n","                    # if np.random.beta(bandit_dict[my_action_list[-1]]['win'], bandit_dict[my_action_list[-1]]['loss']*math.pow(1.03, bandit_dict[my_action_list[-1]]['win']+bandit_dict[my_action_list[-1]]['loss']+bandit_dict[my_action_list[-1]]['opp'])) > 0.5:\r\n","                    if np.random.beta(bandit_dict[my_action_list[-1]]['win'], bandit_dict[my_action_list[-1]]['loss']) > 0.5:\r\n","                        my_pull = my_action_list[-1]\r\n","                    elif observation['step'] > 1000:\r\n","                        my_pull = get_next_bandit_TS()\r\n","                    else:\r\n","                        my_pull = get_next_bandit()\r\n","                elif observation['step'] > 1000:\r\n","                    my_pull = get_next_bandit_TS()\r\n","                    #if random.random() < 0.5:\r\n","                    #    my_pull = get_next_bandit()\r\n","                    #else:\r\n","                    #    my_pull = get_next_bandit_TS()\r\n","                else:\r\n","                  if (op_action_list[-1] == op_action_list[-2]) and (op_action_list[-1] == op_action_list[-3]) and (my_action_list[-1] != op_action_list[-1]):\r\n","                    # if random.random() < 0.6:\r\n","                    # if np.random.beta(bandit_dict[op_action_list[-1]]['win'], bandit_dict[op_action_list[-1]]['loss']*math.pow(1.03, bandit_dict[op_action_list[-1]]['win']+bandit_dict[op_action_list[-1]]['loss']+bandit_dict[op_action_list[-1]]['opp'])) > 0.5:\r\n","                    if np.random.beta(bandit_dict[op_action_list[-1]]['win'], bandit_dict[op_action_list[-1]]['loss']) > 0.5:\r\n","                      my_pull = op_action_list[-1]\r\n","                    else:\r\n","                      my_pull = get_next_bandit()\r\n","                  else:\r\n","                    my_pull = get_next_bandit()\r\n","            else:\r\n","                my_pull = get_next_bandit()\r\n","    \r\n","    return my_pull"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Overwriting pull_same_TS_ucb.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CJcKpIT7kyIz","executionInfo":{"status":"ok","timestamp":1610500782745,"user_tz":-540,"elapsed":1431,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"a7311dbc-2ab0-432a-8c7b-c7ce264fd748"},"source":["%%writefile pull_same_TS_ucb.py\r\n","\r\n","import numpy as np\r\n","import pandas as pd\r\n","import random, os, datetime, math\r\n","from collections import defaultdict\r\n","from scipy.stats import beta\r\n","\r\n","total_reward = 0\r\n","bandit_dict = {}\r\n","\r\n","def set_seed(my_seed=42):\r\n","    os.environ['PYTHONHASHSEED'] = str(my_seed)\r\n","    random.seed(my_seed)\r\n","    np.random.seed(my_seed)\r\n","\r\n","def get_next_bandit():\r\n","    best_bandit = 0\r\n","    best_bandit_expected = 0\r\n","    for bnd in bandit_dict:\r\n","        expect = beta.std(bandit_dict[bnd]['win']*math.pow(0.97, bandit_dict[bnd]['win']+bandit_dict[bnd]['loss']+bandit_dict[bnd]['opp']), bandit_dict[bnd]['loss'])\\\r\n","                + (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5 + bandit_dict[bnd]['op_continue']) \\\r\n","                / (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp']) \\\r\n","                * math.pow(0.97, bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])\r\n","        if expect > best_bandit_expected:\r\n","            best_bandit_expected = expect\r\n","            best_bandit = bnd\r\n","    return best_bandit\r\n","\r\n","def get_next_bandit_TS():\r\n","    best_bandit = 0\r\n","    best_bandit_expected = 0\r\n","    for bnd in bandit_dict:\r\n","        # beta.std(bandit_dict[bnd]['win']*math.pow(0.97, bandit_dict[bnd]['win']+bandit_dict[bnd]['loss']+bandit_dict[bnd]['opp']), bandit_dict[bnd]['loss']) * 3 + \\ \r\n","        # + beta.std(bandit_dict[bnd]['win'],bandit_dict[bnd]['loss']) * 3 \\\r\n","        expect = np.random.beta(bandit_dict[bnd]['win']*math.pow(0.97, bandit_dict[bnd]['win']+bandit_dict[bnd]['loss']+bandit_dict[bnd]['opp']), bandit_dict[bnd]['loss'])/2 \\\r\n","                + (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5 + bandit_dict[bnd]['op_continue']) \\\r\n","                / (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp']) \\\r\n","                * math.pow(0.97, bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])\r\n","        if expect > best_bandit_expected:\r\n","            best_bandit_expected = expect\r\n","            best_bandit = bnd\r\n","    return best_bandit\r\n","\r\n","my_action_list = []\r\n","op_action_list = []\r\n","\r\n","op_continue_cnt_dict = defaultdict(int)\r\n","\r\n","def multi_armed_probabilities(observation, configuration):\r\n","    global total_reward, bandit_dict\r\n","\r\n","    my_pull = 0\r\n","    if 0 == observation['step']:\r\n","        set_seed()\r\n","        total_reward = 0\r\n","        bandit_dict = {}\r\n","        for i in range(configuration['banditCount']):\r\n","            bandit_dict[i] = {'win': 1, 'loss': 1, 'opp': 0, 'my_continue': 0, 'op_continue': 0}\r\n","    else:\r\n","        last_reward = observation['reward'] - total_reward\r\n","        total_reward = observation['reward']\r\n","        \r\n","        my_idx = observation['agentIndex']\r\n","        my_last_action = observation['lastActions'][my_idx]\r\n","        op_last_action = observation['lastActions'][1-my_idx]\r\n","        \r\n","        my_action_list.append(my_last_action)\r\n","        op_action_list.append(op_last_action)\r\n","        \r\n","        if 0 < last_reward:\r\n","            bandit_dict[my_last_action]['win'] = bandit_dict[my_last_action]['win'] +1\r\n","        else:\r\n","            bandit_dict[my_last_action]['loss'] = bandit_dict[my_last_action]['loss'] +1\r\n","        bandit_dict[op_last_action]['opp'] = bandit_dict[op_last_action]['opp'] +1\r\n","        \r\n","        if observation['step'] >= 3:\r\n","            if my_action_list[-1] == my_action_list[-2]:\r\n","                bandit_dict[my_last_action]['my_continue'] += 1\r\n","            else:\r\n","                bandit_dict[my_last_action]['my_continue'] = 0\r\n","            if op_action_list[-1] == op_action_list[-2]:\r\n","                bandit_dict[op_last_action]['op_continue'] += 1\r\n","            else:\r\n","                bandit_dict[op_last_action]['op_continue'] = 0\r\n","        \r\n","        if last_reward > 0:\r\n","            my_pull = my_last_action\r\n","        else:\r\n","            if observation['step'] >= 4:\r\n","              if observation['step'] < 1000:\r\n","                if (my_action_list[-1] == my_action_list[-2]) and (my_action_list[-1] == my_action_list[-3]):\r\n","                    if np.random.beta(bandit_dict[my_action_list[-1]]['win']*math.pow(0.97, bandit_dict[my_action_list[-1]]['win']+bandit_dict[my_action_list[-1]]['loss']+bandit_dict[my_action_list[-1]]['opp']), bandit_dict[my_action_list[-1]]['loss']) > 0.4:\r\n","                        my_pull = my_action_list[-1]\r\n","                    else:\r\n","                        my_pull = get_next_bandit()\r\n","                #elif (op_action_list[-1] == op_action_list[-2]) and (op_action_list[-1] == op_action_list[-3]) and (my_action_list[-1] != op_action_list[-1]):\r\n","                #    if np.random.beta(bandit_dict[op_action_list[-1]]['win']*math.pow(0.97, bandit_dict[op_action_list[-1]]['win']+bandit_dict[op_action_list[-1]]['loss']+bandit_dict[op_action_list[-1]]['opp']), bandit_dict[op_action_list[-1]]['loss']) > 0.6:\r\n","                #      my_pull = op_action_list[-1]\r\n","                #    else:\r\n","                #      my_pull = get_next_bandit()\r\n","                else:\r\n","                    my_pull = get_next_bandit()\r\n","              else:\r\n","                my_pull = get_next_bandit_TS()\r\n","            else:\r\n","                my_pull = get_next_bandit()\r\n","    \r\n","    return my_pull"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting pull_same_TS_ucb.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NK_xDnDLEYOw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610533226328,"user_tz":-540,"elapsed":3772,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"71327744-86f1-459b-b66f-3c4943c375d4"},"source":["!pip install kaggle-environments --upgrade -q"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 112kB 7.1MB/s \n","\u001b[K     |████████████████████████████████| 61kB 5.1MB/s \n","\u001b[31mERROR: nbclient 0.5.1 has requirement jupyter-client>=6.1.5, but you'll have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xh-aLBVJNTXm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610533226329,"user_tz":-540,"elapsed":2530,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"bee2e214-7e35-43c2-9cae-2e7bbd22b659"},"source":["from kaggle_environments import make\r\n","\r\n","env = make(\"mab\", debug=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading environment football failed: No module named 'gfootball'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DfZ2dKbkjRyx","executionInfo":{"status":"ok","timestamp":1610329373672,"user_tz":-540,"elapsed":1871,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"2a3b97cd-5f0e-4fcb-a6c0-0de288d291a3"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["baby-steps-of-rl-ja\t    epsilon_TS_gamma.py  submission.py\n","bayesian_ucb.py\t\t    experiment.py\t thompson_sampling.py\n","bayesian_ucb_TS_epsilon.py  Kaggle_bandit.ipynb  ucb_decay.py\n","bayesian_ucb_TS.py\t    pull_same.py\n","epsilon_TS_beta.py\t    RL_Python.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MmadZoW4JcbW","colab":{"base_uri":"https://localhost:8080/","height":522,"output_embedded_package_id":"1l4Bh7N3MSmHp1zb_snM38e3vWmXHC0Xr"},"executionInfo":{"status":"ok","timestamp":1610533706527,"user_tz":-540,"elapsed":12606,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"2c2ae854-d7f3-4798-87e4-a6c396be1892"},"source":["env.reset()\r\n","# env.run([\"experiment.py\", \"bayesian_ucb.py\"])\r\n","# env.run([\"experiment.py\", \"thompson_sampling.py\"])\r\n","#env.run([\"epsilon_TS_beta.py\", \"bayesian_ucb.py\"])\r\n","#env.run([\"epsilon_TS_beta.py\", \"thompson_sampling.py\"])\r\n","#env.run([\"epsilon_TS_gamma.py\", \"thompson_sampling.py\"])\r\n","#env.run([\"bayesian_ucb_TS.py\", \"bayesian_ucb.py\"])\r\n","#env.run([\"bayesian_ucb_TS_epsilon.py\", \"bayesian_ucb.py\"])\r\n","#env.run([\"bayesian_ucb_TS.py\", \"thompson_sampling.py\"])\r\n","#env.run([\"bayesian_ucb_TS.py\", \"epsilon_TS_beta.py\"])\r\n","#env.run([\"pull_same.py\", \"bayesian_ucb_TS.py\"])\r\n","env.run([\"pull_same_TS_ucb.py\", \"pull_same.py\"])\r\n","env.render(mode=\"ipython\", width=800, height=500)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"wI7iVULAuakL"},"source":["＊実験失敗"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W9isbunkcyV2","executionInfo":{"status":"ok","timestamp":1610280603825,"user_tz":-540,"elapsed":1404,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"3659932e-6bdd-4000-e088-64f34c9f76fe"},"source":["%%writefile bayesian_ucb_TS_epsilon.py\r\n","\r\n","import json\r\n","import numpy as np\r\n","import pandas as pd\r\n","import random\r\n","import math\r\n","from scipy.stats import beta\r\n","\r\n","bandit_state = None\r\n","total_reward = 0\r\n","epsilon = 0.12\r\n","c = 3\r\n","    \r\n","def bayesian_ucb_TS_agent (observation, configuration):\r\n","    global history, history_bandit\r\n","\r\n","    step = 1.0 #you can regulate exploration / exploitation balacne using this param\r\n","    decay_rate = 0.97 # how much do we decay the win count after each call\r\n","    \r\n","    global bandit_state,total_reward, epsilon\r\n","\r\n","    if observation.step == 0:\r\n","        # initial bandit state\r\n","        bandit_state = [[1,1] for i in range(configuration[\"banditCount\"])]\r\n","    else:       \r\n","        # updating bandit_state using the result of the previous step\r\n","        last_reward = observation[\"reward\"] - total_reward\r\n","        total_reward = observation[\"reward\"]\r\n","        \r\n","        # we need to understand who we are Player 1 or 2\r\n","        last_bandit = observation.lastActions[observation.agentIndex]\r\n","\r\n","        if last_reward > 0:\r\n","            bandit_state[last_bandit][0] += last_reward * step\r\n","        else:\r\n","            bandit_state[last_bandit][1] += step\r\n","        \r\n","        bandit_state[observation.lastActions[0]][0] = (bandit_state[observation.lastActions[0]][0] - 1) * decay_rate + 1\r\n","        bandit_state[observation.lastActions[1]][0] = (bandit_state[observation.lastActions[1]][0] - 1) * decay_rate + 1\r\n","\r\n","#     generate random number from Beta distribution for each agent and select the most lucky one   \r\n","    if random.random() < epsilon:\r\n","      if observation.step < 1000:\r\n","        return random.choice(range(configuration[\"banditCount\"]))\r\n","    best_proba = -1\r\n","    best_agent = None\r\n","    for k in range(configuration[\"banditCount\"]):\r\n","        if observation.step < 500:\r\n","          proba = bandit_state[k][0] / (bandit_state[k][0]+bandit_state[k][1]) + beta.std(bandit_state[k][0],bandit_state[k][1]) * c\r\n","        else:\r\n","          proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\r\n","        if proba > best_proba:\r\n","            best_proba = proba\r\n","            best_agent = k\r\n","\r\n","    return best_agent"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting bayesian_ucb_TS_epsilon.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"axNVAkKvJEyh","executionInfo":{"status":"ok","timestamp":1610202477458,"user_tz":-540,"elapsed":623,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"190658aa-46f7-413d-baa6-f65f5c2150dd"},"source":["%%writefile epsilon_TS_gamma.py\r\n","\r\n","import json\r\n","import numpy as np\r\n","import pandas as pd\r\n","import random\r\n","\r\n","bandit_state = None\r\n","total_reward = 0\r\n","epsilon = 0.1\r\n","    \r\n","def epsilon_TS_gamma_agent (observation, configuration):\r\n","    global history, history_bandit\r\n","\r\n","    step = 1.0 #you can regulate exploration / exploitation balacne using this param\r\n","    decay_rate = 0.97 # how much do we decay the win count after each call\r\n","    \r\n","    global bandit_state,total_reward, epsilon\r\n","\r\n","    if observation.step == 0:\r\n","        # initial bandit state\r\n","        bandit_state = [[1,1] for i in range(configuration[\"banditCount\"])]\r\n","    else:       \r\n","        # updating bandit_state using the result of the previous step\r\n","        last_reward = observation[\"reward\"] - total_reward\r\n","        total_reward = observation[\"reward\"]\r\n","        \r\n","        # we need to understand who we are Player 1 or 2\r\n","        last_bandit = observation.lastActions[observation.agentIndex]\r\n","\r\n","        if last_reward > 0:\r\n","            bandit_state[last_bandit][0] += last_reward * step\r\n","        else:\r\n","            bandit_state[last_bandit][1] += step\r\n","        \r\n","        bandit_state[observation.lastActions[0]][0] = (bandit_state[observation.lastActions[0]][0] - 1) * decay_rate + 1\r\n","        bandit_state[observation.lastActions[1]][0] = (bandit_state[observation.lastActions[1]][0] - 1) * decay_rate + 1\r\n","\r\n","\r\n","    if random.random() < epsilon:\r\n","      if observation.step < 1000:\r\n","        return random.choice(range(configuration[\"banditCount\"]))\r\n","#     generate random number from Beta distribution for each agent and select the most lucky one        \r\n","    best_proba = -1\r\n","    best_agent = None\r\n","    for k in range(configuration[\"banditCount\"]):\r\n","        proba = np.random.gamma(bandit_state[k][0],bandit_state[k][1])\r\n","        if proba > best_proba:\r\n","            best_proba = proba\r\n","            best_agent = k\r\n","\r\n","    return best_agent"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing epsilon_TS_gamma.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z0mHSdg2M1sE","executionInfo":{"status":"ok","timestamp":1610098015433,"user_tz":-540,"elapsed":741,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"03bda575-4f1b-47e8-adc0-92b0af572386"},"source":["%%writefile experiment.py\r\n","\r\n","import math\r\n","import numpy as np\r\n","from scipy.stats import beta\r\n","\r\n","win, lose, bandit, root = [None] * 4\r\n","total_reward = 0\r\n","c = 3\r\n","\r\n","def agent(observation, configuration):\r\n","    global total_reward, bandit, win, lose, c, root\r\n","\r\n","    if observation.step == 0:\r\n","        win, lose, root = np.ones((3, configuration.banditCount))\r\n","    else:\r\n","        r = observation.reward - total_reward\r\n","        total_reward = observation.reward\r\n","        # Update Gaussian posterior\r\n","        win[bandit] += r\r\n","        lose[bandit] += 1 - r\r\n","        root[bandit] = math.sqrt(win[bandit]+lose[bandit])\r\n","\r\n","    bound = win / (win + lose) + beta.std(win, lose) * c / root\r\n","    bandit = int(np.argmax(bound))\r\n","    \r\n","    return bandit"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting experiment.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M7DWNi-VGutE"},"source":["＊初期設定のテスト"]},{"cell_type":"code","metadata":{"id":"mm1IxOxmHtiY"},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ph8iaCImDzca","executionInfo":{"status":"ok","timestamp":1610330910408,"user_tz":-540,"elapsed":749,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"d63d075d-103c-4213-e61a-0b7492632f17"},"source":["%%writefile pull_same.py\r\n","\r\n","import numpy as np\r\n","import pandas as pd\r\n","import random, os, datetime, math\r\n","from collections import defaultdict\r\n","\r\n","total_reward = 0\r\n","bandit_dict = {}\r\n","\r\n","def set_seed(my_seed=42):\r\n","    os.environ['PYTHONHASHSEED'] = str(my_seed)\r\n","    random.seed(my_seed)\r\n","    np.random.seed(my_seed)\r\n","\r\n","def get_next_bandit():\r\n","    best_bandit = 0\r\n","    best_bandit_expected = 0\r\n","    for bnd in bandit_dict:\r\n","        expect = (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5 + bandit_dict[bnd]['op_continue']) \\\r\n","                 / (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp']) \\\r\n","                * math.pow(0.97, bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])\r\n","        #print(\"1:\\t\", expect)\r\n","        a = bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5 + bandit_dict[bnd]['op_continue']\r\n","        b = bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp']\r\n","        c = math.pow(0.97, bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])\r\n","        #print(\"2:\\t\", a+a/b*c)\r\n","        #print(\"3:\\t\", (a/b)*c)\r\n","        #print(\"3:\\t\", a+(a/b*c))\r\n","        if expect > best_bandit_expected:\r\n","            best_bandit_expected = expect\r\n","            best_bandit = bnd\r\n","    return best_bandit\r\n","\r\n","my_action_list = []\r\n","op_action_list = []\r\n","\r\n","op_continue_cnt_dict = defaultdict(int)\r\n","\r\n","def multi_armed_probabilities(observation, configuration):\r\n","    global total_reward, bandit_dict\r\n","\r\n","    my_pull = random.randrange(configuration['banditCount'])\r\n","    if 0 == observation['step']:\r\n","        set_seed()\r\n","        total_reward = 0\r\n","        bandit_dict = {}\r\n","        for i in range(configuration['banditCount']):\r\n","            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0, 'my_continue': 0, 'op_continue': 0}\r\n","    else:\r\n","        last_reward = observation['reward'] - total_reward\r\n","        total_reward = observation['reward']\r\n","        \r\n","        my_idx = observation['agentIndex']\r\n","        my_last_action = observation['lastActions'][my_idx]\r\n","        op_last_action = observation['lastActions'][1-my_idx]\r\n","        \r\n","        my_action_list.append(my_last_action)\r\n","        op_action_list.append(op_last_action)\r\n","        \r\n","        if 0 < last_reward:\r\n","            bandit_dict[my_last_action]['win'] = bandit_dict[my_last_action]['win'] +1\r\n","        else:\r\n","            bandit_dict[my_last_action]['loss'] = bandit_dict[my_last_action]['loss'] +1\r\n","        bandit_dict[op_last_action]['opp'] = bandit_dict[op_last_action]['opp'] +1\r\n","        \r\n","        if observation['step'] >= 3:\r\n","            if my_action_list[-1] == my_action_list[-2]:\r\n","                bandit_dict[my_last_action]['my_continue'] += 1\r\n","            else:\r\n","                bandit_dict[my_last_action]['my_continue'] = 0\r\n","            if op_action_list[-1] == op_action_list[-2]:\r\n","                bandit_dict[op_last_action]['op_continue'] += 1\r\n","            else:\r\n","                bandit_dict[op_last_action]['op_continue'] = 0\r\n","        \r\n","        if last_reward > 0:\r\n","            my_pull = my_last_action\r\n","        else:\r\n","            if observation['step'] >= 4:\r\n","                if (my_action_list[-1] == my_action_list[-2]) and (my_action_list[-1] == my_action_list[-3]):\r\n","                    if random.random() < 0.5:\r\n","                        my_pull = my_action_list[-1]\r\n","                    else:\r\n","                        my_pull = get_next_bandit()\r\n","                else:\r\n","                    my_pull = get_next_bandit()\r\n","            else:\r\n","                my_pull = get_next_bandit()\r\n","    \r\n","    return my_pull"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting pull_same.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xHAbUvQkE_OY","executionInfo":{"status":"ok","timestamp":1610097453714,"user_tz":-540,"elapsed":645,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"743de489-f4dd-454f-fa0b-fb1799ce4e51"},"source":["%%writefile ucb_decay.py\r\n","\r\n","import numpy as np\r\n","\r\n","decay = 0.97\r\n","total_reward = 0\r\n","bandit = None\r\n","\r\n","def agent(observation, configuration):\r\n","    #print(observation)\r\n","    #if observation.step == 100:\r\n","    #  print(float(observation[\"reward\"]))\r\n","    #  print(observation[\"lastActions\"][1-observation.agentIndex])      \r\n","    if observation.step == 0:\r\n","    #  print(\"------------test---------------\")\r\n","    #  print(observation)\r\n","      print(configuration)\r\n","    #if observation[\"step\"] == 0:\r\n","    #  print(\"------------test---------------\")\r\n","    global reward_sums, n_selections, total_reward, bandit\r\n","    \r\n","    n_bandits = configuration.banditCount\r\n","\r\n","    if observation.step == 0:\r\n","        n_selections, reward_sums = np.full((2, n_bandits), 1e-32)\r\n","    else:\r\n","        #print(observation.step, \": \", observation.reward - total_reward)\r\n","        reward_sums[bandit] += decay * (observation.reward - total_reward)\r\n","        total_reward = observation.reward\r\n","\r\n","    avg_reward = reward_sums / n_selections    \r\n","    delta_i = np.sqrt(2 * np.log(observation.step + 1) / n_selections)\r\n","    bandit = int(np.argmax(avg_reward + delta_i))\r\n","\r\n","    n_selections[bandit] += 1\r\n","\r\n","    return bandit"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting ucb_decay.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VAD9g_d9FuVv","executionInfo":{"status":"ok","timestamp":1609763520708,"user_tz":-540,"elapsed":933,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"63d113d9-1736-4277-bbdb-6bba06a9c10c"},"source":["%%writefile bayesian_ucb.py\r\n","\r\n","import numpy as np\r\n","from scipy.stats import beta\r\n","\r\n","post_a, post_b, bandit = [None] * 3\r\n","total_reward = 0\r\n","c = 3\r\n","\r\n","def agent(observation, configuration):\r\n","    global total_reward, bandit, post_a, post_b, c\r\n","\r\n","    if observation.step == 0:\r\n","        post_a, post_b = np.ones((2, configuration.banditCount))\r\n","    else:\r\n","        r = observation.reward - total_reward\r\n","        total_reward = observation.reward\r\n","        # Update Gaussian posterior\r\n","        post_a[bandit] += r\r\n","        post_b[bandit] += 1 - r\r\n","    \r\n","    bound = post_a / (post_a + post_b) + beta.std(post_a, post_b) * c\r\n","    bandit = int(np.argmax(bound))\r\n","    \r\n","    return bandit"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting bayesian_ucb.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8eXGmlxeK0Bm"},"source":["%%writefile thompson_sampling.py\r\n","\r\n","import json\r\n","import numpy as np\r\n","import pandas as pd\r\n","\r\n","bandit_state = None\r\n","total_reward = 0\r\n","last_step = None\r\n","    \r\n","def multi_armed_bandit_agent (observation, configuration):\r\n","    global history, history_bandit\r\n","\r\n","    step = 1.0 #you can regulate exploration / exploitation balacne using this param\r\n","    decay_rate = 0.97 # how much do we decay the win count after each call\r\n","    \r\n","    global bandit_state,total_reward,last_step\r\n","        \r\n","    if observation.step == 0:\r\n","        # initial bandit state\r\n","        bandit_state = [[1,1] for i in range(configuration[\"banditCount\"])]\r\n","    else:       \r\n","        # updating bandit_state using the result of the previous step\r\n","        last_reward = observation[\"reward\"] - total_reward\r\n","        total_reward = observation[\"reward\"]\r\n","        \r\n","        # we need to understand who we are Player 1 or 2\r\n","        player = int(last_step == observation.lastActions[1])\r\n","        \r\n","        if last_reward > 0:\r\n","            bandit_state[observation.lastActions[player]][0] += last_reward * step\r\n","        else:\r\n","            bandit_state[observation.lastActions[player]][1] += step\r\n","        \r\n","        bandit_state[observation.lastActions[0]][0] = (bandit_state[observation.lastActions[0]][0] - 1) * decay_rate + 1\r\n","        bandit_state[observation.lastActions[1]][0] = (bandit_state[observation.lastActions[1]][0] - 1) * decay_rate + 1\r\n","\r\n","#     generate random number from Beta distribution for each agent and select the most lucky one\r\n","    best_proba = -1\r\n","    best_agent = None\r\n","    for k in range(configuration[\"banditCount\"]):\r\n","        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\r\n","        if proba > best_proba:\r\n","            best_proba = proba\r\n","            best_agent = k\r\n","        \r\n","    last_step = best_agent\r\n","    return best_agent"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ocEXKhkFJeP"},"source":["from kaggle_environments import make\r\n","\r\n","env = make(\"mab\", debug=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7IxCU2AUFN1y","colab":{"base_uri":"https://localhost:8080/","height":522,"output_embedded_package_id":"1L-__CbBc50EM-T5g7eJ1Yh5fc8k5QNwU"},"executionInfo":{"status":"ok","timestamp":1610098615034,"user_tz":-540,"elapsed":11918,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"55cc99b5-3bdb-4005-9fc7-bdef720e5f89"},"source":["env.reset()\r\n","#env.run([\"ucb_decay.py\", \"bayesian_ucb.py\"])\r\n","env.run([\"bayesian_ucb.py\", \"thompson_sampling.py\"])\r\n","env.render(mode=\"ipython\", width=800, height=500)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"-SYXJ4prGm8Z"},"source":["＊初期設定"]},{"cell_type":"code","metadata":{"id":"cJn7943-Ch-q"},"source":["!pip install kaggle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"prKlWytnD-pR"},"source":["from google.colab import files\r\n","files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZLvRKBpELqx"},"source":["!mkdir -p ~/.kaggle\r\n","!mv kaggle.json ~/.kaggle/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fAWM4QzECigB"},"source":["# download API key from google drive\r\n","## Original: https://colab.research.google.com/drive/1eufc8aNCdjHbrBhuy7M7X6BGyzAyRbrF#scrollTo=y5_288BYp6H1\r\n","## When you run for the first time, you will see a link to authenticate.\r\n","\r\n","from googleapiclient.discovery import build\r\n","import io, os\r\n","from googleapiclient.http import MediaIoBaseDownload\r\n","from google.colab import auth\r\n","\r\n","auth.authenticate_user()\r\n","\r\n","drive_service = build('drive', 'v3')\r\n","results = drive_service.files().list(\r\n","        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\r\n","kaggle_api_key = results.get('files', [])\r\n","\r\n","filename = \"/root/.kaggle/kaggle.json\"\r\n","os.makedirs(os.path.dirname(filename), exist_ok=True)\r\n","\r\n","request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\r\n","fh = io.FileIO(filename, 'wb')\r\n","downloader = MediaIoBaseDownload(fh, request)\r\n","done = False\r\n","while done is False:\r\n","    status, done = downloader.next_chunk()\r\n","    print(\"Download %d%%.\" % int(status.progress() * 100))\r\n","os.chmod(filename, 600)"],"execution_count":null,"outputs":[]}]}