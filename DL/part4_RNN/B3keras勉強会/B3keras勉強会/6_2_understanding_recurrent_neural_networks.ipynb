{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"6_2_understanding_recurrent_neural_networks.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"mySxR9f_bXBK","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1615263860918,"user_tz":-540,"elapsed":3471,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"2919c067-89b6-4c91-bf24-6144ccab72fb"},"source":["import keras\n","keras.__version__"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.4.3'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"5_pMV9zTbXBM"},"source":["# Understanding recurrent neural networks\n","\n","This notebook contains the code samples found in Chapter 6, Section 2 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n","\n","---\n","\n","[...]\n","\n","## A first recurrent layer in Keras\n","\n","The process we just naively implemented in Numpy corresponds to an actual Keras layer: the `SimpleRNN` layer:\n"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"6oR48ApdbXBN","executionInfo":{"status":"ok","timestamp":1615263864167,"user_tz":-540,"elapsed":1665,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}}},"source":["from keras.layers import SimpleRNN"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pWGO550CbXBN"},"source":["There is just one minor difference: `SimpleRNN` processes batches of sequences, like all other Keras layers, not just a single sequence like \n","in our Numpy example. This means that it takes inputs of shape `(batch_size, timesteps, input_features)`, rather than `(timesteps, \n","input_features)`.\n","\n","Like all recurrent layers in Keras, `SimpleRNN` can be run in two different modes: it can return either the full sequences of successive \n","outputs for each timestep (a 3D tensor of shape `(batch_size, timesteps, output_features)`), or it can return only the last output for each \n","input sequence (a 2D tensor of shape `(batch_size, output_features)`). These two modes are controlled by the `return_sequences` constructor \n","argument. Let's take a look at an example:"]},{"cell_type":"code","metadata":{"id":"GsZZpY0xbXBO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615263872619,"user_tz":-540,"elapsed":6656,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"1dd230dd-4c4e-44c6-8567-328afb12f3d8"},"source":["from keras.models import Sequential\n","from keras.layers import Embedding, SimpleRNN\n","\n","model = Sequential()\n","model.add(Embedding(10000, 32))\n","model.add(SimpleRNN(32))\n","model.summary()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, None, 32)          320000    \n","_________________________________________________________________\n","simple_rnn (SimpleRNN)       (None, 32)                2080      \n","=================================================================\n","Total params: 322,080\n","Trainable params: 322,080\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GmWVkPsnbXBO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615264062056,"user_tz":-540,"elapsed":553,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"d8e0de77-e74f-4a22-8fe3-fb7d3d0109d6"},"source":["model = Sequential()\n","model.add(Embedding(10000, 32))\n","model.add(SimpleRNN(32, return_sequences=True))\n","model.summary()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, None, 32)          320000    \n","_________________________________________________________________\n","simple_rnn_1 (SimpleRNN)     (None, None, 32)          2080      \n","=================================================================\n","Total params: 322,080\n","Trainable params: 322,080\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wGofR8TvbXBP"},"source":["It is sometimes useful to stack several recurrent layers one after the other in order to increase the representational power of a network. \n","In such a setup, you have to get all intermediate layers to return full sequences:"]},{"cell_type":"code","metadata":{"id":"L6xWORpHbXBP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615264115748,"user_tz":-540,"elapsed":664,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"993a2083-4e95-4929-96da-79e7e33da888"},"source":["model = Sequential()\n","model.add(Embedding(10000, 32))\n","model.add(SimpleRNN(32, return_sequences=True))\n","model.add(SimpleRNN(32, return_sequences=True))\n","model.add(SimpleRNN(32, return_sequences=True))\n","model.add(SimpleRNN(32))  # This last layer only returns the last outputs.\n","model.summary()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, None, 32)          320000    \n","_________________________________________________________________\n","simple_rnn_2 (SimpleRNN)     (None, None, 32)          2080      \n","_________________________________________________________________\n","simple_rnn_3 (SimpleRNN)     (None, None, 32)          2080      \n","_________________________________________________________________\n","simple_rnn_4 (SimpleRNN)     (None, None, 32)          2080      \n","_________________________________________________________________\n","simple_rnn_5 (SimpleRNN)     (None, 32)                2080      \n","=================================================================\n","Total params: 328,320\n","Trainable params: 328,320\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5af-CQIHbXBQ"},"source":["Now let's try to use such a model on the IMDB movie review classification problem. First, let's preprocess the data:"]},{"cell_type":"code","metadata":{"id":"cH5S6HGkbXBQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615264230098,"user_tz":-540,"elapsed":7006,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"1062aeed-3aed-43d2-fe77-2005b20e64d5"},"source":["from keras.datasets import imdb\n","from keras.preprocessing import sequence\n","\n","max_features = 10000  # number of words to consider as features\n","maxlen = 500  # cut texts after this number of words (among top max_features most common words)\n","batch_size = 32\n","\n","print('Loading data...')\n","(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n","print(len(input_train), 'train sequences')\n","print(len(input_test), 'test sequences')\n","\n","print('Pad sequences (samples x time)')\n","input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n","input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n","print('input_train shape:', input_train.shape)\n","print('input_test shape:', input_test.shape)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Loading data...\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n"],"name":"stdout"},{"output_type":"stream","text":["<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"],"name":"stderr"},{"output_type":"stream","text":["25000 train sequences\n","25000 test sequences\n","Pad sequences (samples x time)\n","input_train shape: (25000, 500)\n","input_test shape: (25000, 500)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uHS1arONbXBQ"},"source":["Let's train a simple recurrent network using an `Embedding` layer and a `SimpleRNN` layer:"]},{"cell_type":"code","metadata":{"id":"PWJXGK0-bXBQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615264756839,"user_tz":-540,"elapsed":503919,"user":{"displayName":"Koya Kokubu","photoUrl":"","userId":"03447341678313529916"}},"outputId":"4a63d38a-5819-4c89-b3ae-11426978b8c8"},"source":["from keras.layers import Dense\n","\n","model = Sequential()\n","model.add(Embedding(max_features, 32))\n","model.add(SimpleRNN(32))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","history = model.fit(input_train, y_train,\n","                    epochs=10,\n","                    batch_size=128,\n","                    validation_split=0.2)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","157/157 [==============================] - 55s 326ms/step - loss: 0.6705 - acc: 0.5676 - val_loss: 0.4295 - val_acc: 0.8186\n","Epoch 2/10\n","157/157 [==============================] - 50s 319ms/step - loss: 0.4103 - acc: 0.8233 - val_loss: 0.5790 - val_acc: 0.7294\n","Epoch 3/10\n","157/157 [==============================] - 50s 319ms/step - loss: 0.3592 - acc: 0.8579 - val_loss: 0.4035 - val_acc: 0.8418\n","Epoch 4/10\n","157/157 [==============================] - 50s 319ms/step - loss: 0.2496 - acc: 0.9038 - val_loss: 0.3852 - val_acc: 0.8300\n","Epoch 5/10\n","157/157 [==============================] - 50s 322ms/step - loss: 0.1783 - acc: 0.9357 - val_loss: 0.4970 - val_acc: 0.8074\n","Epoch 6/10\n","157/157 [==============================] - 50s 317ms/step - loss: 0.1314 - acc: 0.9564 - val_loss: 0.4059 - val_acc: 0.8554\n","Epoch 7/10\n","157/157 [==============================] - 49s 315ms/step - loss: 0.0786 - acc: 0.9736 - val_loss: 0.4885 - val_acc: 0.8176\n","Epoch 8/10\n","157/157 [==============================] - 49s 314ms/step - loss: 0.0411 - acc: 0.9892 - val_loss: 0.6648 - val_acc: 0.7514\n","Epoch 9/10\n","157/157 [==============================] - 49s 311ms/step - loss: 0.0291 - acc: 0.9924 - val_loss: 0.5657 - val_acc: 0.8302\n","Epoch 10/10\n","157/157 [==============================] - 50s 322ms/step - loss: 0.0173 - acc: 0.9955 - val_loss: 0.6669 - val_acc: 0.7966\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fE9Ja_UDbXBR"},"source":["Let's display the training and validation loss and accuracy:"]},{"cell_type":"code","metadata":{"id":"JfQ_YJumbXBR"},"source":["import matplotlib.pyplot as plt\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tfqm5Jgg61Lq","outputId":"fa0ecf03-28e7-476c-a2c3-552bfce2bf39"},"source":["results = model.evaluate(input_test, y_test, batch_size=512)\n","print(\"Accuracy = \", results[1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["49/49 [==============================] - 14s 279ms/step - loss: 0.4188 - acc: 0.8647\n","Accuracy =  0.8647199869155884\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rAIXpXo7bXBR"},"source":["As a reminder, in chapter 3, our very first naive approach to this very dataset got us to 88% test accuracy. Unfortunately, our small \n","recurrent network doesn't perform very well at all compared to this baseline (only up to 85% validation accuracy). Part of the problem is \n","that our inputs only consider the first 500 words rather the full sequences -- \n","hence our RNN has access to less information than our earlier baseline model. The remainder of the problem is simply that `SimpleRNN` isn't very good at processing long sequences, like text. Other types of recurrent layers perform much better. Let's take a look at some \n","more advanced layers."]},{"cell_type":"markdown","metadata":{"id":"SUZMiW57bXBS"},"source":["[...]\n","\n","## A concrete LSTM example in Keras\n","\n","Now let's switch to more practical concerns: we will set up a model using a LSTM layer and train it on the IMDB data. Here's the network, \n","similar to the one with `SimpleRNN` that we just presented. We only specify the output dimensionality of the LSTM layer, and leave every \n","other argument (there are lots) to the Keras defaults. Keras has good defaults, and things will almost always \"just work\" without you \n","having to spend time tuning parameters by hand."]},{"cell_type":"code","metadata":{"id":"KCSZtJzZbXBS"},"source":["from keras.layers import LSTM\n","\n","model = Sequential()\n","model.add(Embedding(max_features, 32))\n","model.add(LSTM(32))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop',\n","              loss='binary_crossentropy',\n","              metrics=['acc'])\n","history = model.fit(input_train, y_train,\n","                    epochs=10,\n","                    batch_size=128,\n","                    validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q3bcUMeRbXBS"},"source":["acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KpkbPAm61dHr"},"source":["results = model.evaluate(input_test, y_test, batch_size=512)\n","print(\"Accuracy = \", results[1])"],"execution_count":null,"outputs":[]}]}